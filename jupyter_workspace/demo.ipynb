{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PyNutrien Demo/Development notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1) Install AWS CLI and set secrets in `~/.aws/credentials`\n",
    "2) Set profile region in `~/.aws/config` -- TODO see link\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/glue_user/workspace', '/home/glue_user/workspace/jupyter_workspace', '/home/glue_user/aws-glue-libs/PyGlue.zip', '/home/glue_user/spark/python/lib/py4j-0.10.9-src.zip', '/home/glue_user/spark/python', '/usr/lib64/python37.zip', '/usr/lib64/python3.7', '/usr/lib64/python3.7/lib-dynload', '', '/home/glue_user/.local/lib/python3.7/site-packages', '/usr/lib64/python3.7/site-packages', '/usr/lib/python3.7/site-packages', '/home/glue_user/.local/lib/python3.7/site-packages/IPython/extensions', '/home/glue_user/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "#lib_path = str(Path(os.getcwd()).parent.absolute().resolve())\n",
    "#sys.path.insert(0, lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fd5dbe678052:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1-amzn-0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6b4462bbd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pynutrien\n",
    "\n",
    "from pynutrien.aws.glue import GlueSparkContext\n",
    "\n",
    "context = GlueSparkContext()\n",
    "context.spark_session # show SparkUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynutrien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalog Reader Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+-------------+----------+---------------+--------------+-----+-----------------+------------+-------+----------------+----+\n",
      "|primary_key|       name|gender|customer_type|birth_date|    personal_id|   card_number|email|     phone_number|country_code|address|meta_last_update|year|\n",
      "+-----------+-----------+------+-------------+----------+---------------+--------------+-----+-----------------+------------+-------+----------------+----+\n",
      "|     135567|  T. Oakman|      |       Active|          | SIN: 154720759|Not Applicable|     |   1 822 435 3970|          CA|       |      2008-08-12|2017|\n",
      "|     135635|B. Kanahele|      |     Prospect|1959-08-20| SIN: 755909975|Not Applicable|     |            -2865|          CA|       |      2008-08-08|2017|\n",
      "|     135844|  B. Wandel|     M|       Active|1956-06-15|NINO: VT931050C|Not Applicable|     |                 |          CA|       |      2008-01-05|2017|\n",
      "|     136073|    S. Naff|     M|       Active|1981-06-22| SSN: 817967572|Not Applicable|     |+1 (254) 817 5954|          US|       |      2008-08-04|2017|\n",
      "|     136304| F. Dolejsi|      |     Prospect|1970-01-31| SSN: 867689822|   5.58434E+15|     |   (101) 525 4703|          US|       |      2008-04-08|2017|\n",
      "|     136501|   C. Gudis|      |       Active|1991-07-31|NINO: BI502209D|Not Applicable|     |      0787 285898|          CA|       |      2008-03-03|2017|\n",
      "|     136517|K. Savickas|      |       Active|1971-10-20| SIN: 906953997|   4.18434E+15|     | 1 (236) 068 7996|          CA|       |      2008-04-04|2017|\n",
      "|     136614|  L. Denmon|      |       Active|1958-09-09| SIN: 616285730|   5.58534E+15|     |                 |          CA|       |      2008-05-09|2017|\n",
      "|     136620|    S. Mott|      |     Prospect|1962-12-10| SSN: 565252427|Not Applicable|     |   1 361 671 7691|          US|       |      2008-06-02|2017|\n",
      "|     136654|  G. Watkin|      |       Active|1963-02-13| SSN: 761634015|Not Applicable|     |      16623613508|          US|       |      2008-06-02|2017|\n",
      "|     136757|   M. Luman|      |       Active|          |NINO: XQ494342D|Not Applicable|     |                 |          CA|       |      2008-01-05|2017|\n",
      "|     136780| R. Burrier|      |     Prospect|       N/A|NINO: WX221478B|Not Applicable|     |    059 2556 6016|          CA|       |      2008-11-07|2017|\n",
      "|     136999|   T. Rhame|      |     Prospect|          |NINO: RS587408B|   5.58534E+15|     |                 |          CA|       |      2008-02-06|2017|\n",
      "|     137028|    A. Toti|      |       Active|          |               |   5.58534E+15|     |   (398) 274 7233|          CA|       |      2008-04-08|2017|\n",
      "|     137041|  R. Maciag|      |       Active|1966-04-13| SIN: 017333360|   5.58534E+15|     |                 |          CA|       |      2008-09-09|2017|\n",
      "|     137141|  G. Holien|      |       Active|1978-11-24| SSN: 325525732|Not Applicable|     |           -10369|          US|       |      2008-09-09|2017|\n",
      "|     137286|  H. Shanor|      |       Active|1996-06-26| SSN: 736656754|   5.58634E+15|     |   1-989-077-9775|          US|       |      2008-01-09|2017|\n",
      "|     137403|    M. Yust|      |     Prospect|       N/A| SIN: 489616383|   5.58634E+15|     |   1 970 510 9387|          CA|       |      2008-01-01|2017|\n",
      "|     137451| S. Blazing|      |       Active|1985-06-06| SSN: 508179127|   4.18634E+15|     |   (009) 408 7671|          US|       |      2008-11-03|2017|\n",
      "|     137476| M. Lashmet|      |       Active|1959-02-23| SSN: 700051470|Not Applicable|     |                 |          US|       |      2008-09-09|2017|\n",
      "+-----------+-----------+------+-------------+----------+---------------+--------------+-----+-----------------+------------+-------+----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pynutrien.aws.glue import GlueSparkContext\n",
    "from pynutrien.aws.glue.catalog import GlueReader\n",
    "\n",
    "context = GlueSparkContext()\n",
    "Reader = GlueReader(context.glue_context, None)\n",
    "df = (\n",
    "    Reader.catalog(\"crawler_test\", \"caseware_customer_a\")\n",
    "        .transformation_ctx(\"cases\")\n",
    "        .dataframe()\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add arguments\n",
    "import sys\n",
    "sys.argv.extend([\"--cfg_file_path\", \"dummy\", \"--env_file_path\", \"dummy\", \"--abc\", \"123\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below part of the code can be only run from docker container. Run the 'util/docker_run_notebook.sh' first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 14:58:27.318 [INFO] [credentials:1223] MainThread:botocore.credentials - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2022-11-30 14:58:27.610 [INFO] [logger:84] MainThread:TEST_GLUE - Starting: TEST_GLUE\n",
      "2022-11-30 14:58:27.611 [INFO] [logger:84] MainThread:TEST_GLUE - Starting: setup\n",
      "2022-11-30 14:58:27.614 [INFO] [job:79] MainThread:TEST_GLUE - Supplied Arguments: ['/home/glue_user/.local/lib/python3.7/site-packages/ipykernel_launcher.py', '-f', '/home/glue_user/.local/share/jupyter/runtime/kernel-355a1265-2c57-4bab-9cf3-915a95661778.json', '--cfg_file_path', '', '--env_file_path', '', '--abc', '123', '--cfg_file_path', 'dummy', '--env_file_path', 'dummy', '--abc', '123']\n",
      "2022-11-30 14:58:27.615 [INFO] [job:80] MainThread:TEST_GLUE - Parsed Arguments: {'job_bookmark_option': 'job-bookmark-disable', 'job_bookmark_from': None, 'job_bookmark_to': None, 'JOB_ID': None, 'JOB_RUN_ID': None, 'SECURITY_CONFIGURATION': None, 'encryption_type': None, 'enable_data_lineage': None, 'RedshiftTempDir': None, 'TempDir': None, 'abc': '123', 'env_file_path': 'dummy', 'cfg_file_path': 'dummy'}\n",
      "2022-11-30 14:58:27.656 [INFO] [job:83] MainThread:TEST_GLUE - Spark Config: [('spark.eventLog.enabled', 'true'), ('spark.network.crypto.keyLength', '256'), ('spark.network.crypto.enabled', 'true'), ('spark.network.crypto.keyFactoryAlgorithm', 'PBKDF2WithHmacSHA256'), ('spark.io.encryption.enabled', 'false'), ('spark.driver.host', 'fd5dbe678052'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.deployMode', 'client'), ('spark.driver.port', '44353'), ('spark.sql.warehouse.dir', 'file:/home/glue_user/workspace/jupyter_workspace/spark-warehouse'), ('spark.executor.extraClassPath', '/home/glue_user/spark/jars/*:/home/glue_user/aws-glue-libs/jars/*'), ('spark.executor.id', 'driver'), ('spark.app.id', 'local-1669819871649'), ('spark.authenticate.secret', '060b93c5-0bf9-494e-a604-766940b1a152'), ('spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs', 'false'), ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'), ('spark.unsafe.sorter.spill.read.ahead.enabled', 'false'), ('spark.app.name', 'pyspark-shell'), ('spark.history.fs.logDirectory', 'file:////tmp/spark-events'), ('spark.sql.catalogImplementation', 'hive'), ('spark.rdd.compress', 'True'), ('spark.app.startTime', '1669819870920'), ('spark.network.crypto.saslFallback', 'false'), ('spark.submit.pyFiles', ''), ('spark.driver.extraClassPath', '/home/glue_user/spark/jars/*:/home/glue_user/aws-glue-libs/jars/*'), ('spark.ui.showConsoleProgress', 'true'), ('spark.authenticate', 'true')]\n",
      "2022-11-30 14:58:27.661 [INFO] [job:116] MainThread:TEST_GLUE - Parsed Env: {}\n",
      "2022-11-30 14:58:27.662 [INFO] [job:118] MainThread:TEST_GLUE - Parsed Config: {}\n",
      "2022-11-30 14:58:27.663 [INFO] [logger:88] MainThread:TEST_GLUE - End: setup [0.052s]\n",
      "2022-11-30 14:58:27.663 [INFO] [logger:84] MainThread:TEST_GLUE - Starting: extract\n",
      "2022-11-30 14:58:28.173 [INFO] [logger:88] MainThread:TEST_GLUE - End: extract [0.510s]\n",
      "2022-11-30 14:58:28.175 [INFO] [logger:84] MainThread:TEST_GLUE - Starting: transform\n",
      "2022-11-30 14:58:28.175 [INFO] [logger:88] MainThread:TEST_GLUE - End: transform [0.001s]\n",
      "2022-11-30 14:58:28.176 [INFO] [logger:84] MainThread:TEST_GLUE - Starting: load\n",
      "2022-11-30 14:58:28.177 [INFO] [logger:88] MainThread:TEST_GLUE - End: load [0.001s]\n",
      "2022-11-30 14:58:28.177 [INFO] [logger:84] MainThread:TEST_GLUE - Starting: cleanup\n",
      "2022-11-30 14:58:28.184 [INFO] [logger:88] MainThread:TEST_GLUE - End: cleanup [0.007s]\n",
      "2022-11-30 14:58:28.186 [INFO] [logger:88] MainThread:TEST_GLUE - End: TEST_GLUE [0.576s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pynutrien.aws.glue import GlueJob\n",
    "\n",
    "class MyGlueJob(GlueJob):\n",
    "    job_name = \"TEST_GLUE\"\n",
    "    arguments = [\"abc\"]\n",
    "    input_path=\"s3://awsglue-datasets/examples/us-legislators/all/persons.json\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def extract(self,):\n",
    "        \n",
    "        self.dynamicframe = self.glue_context.create_dynamic_frame.from_options(\n",
    "            connection_type=\"s3\",\n",
    "            connection_options={\"paths\": [self.input_path], \"recurse\": True},\n",
    "            format=\"json\",\n",
    "        )\n",
    "\n",
    "    def transform(self):\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "job = MyGlueJob()\n",
    "job.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 15:04:59.389 [INFO] [credentials:1223] MainThread:botocore.credentials - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from pynutrien.aws.dynamodb import DynamoDB,DynamoDBPandas\n",
    "\n",
    "session = boto3.session.Session()\n",
    "client = session.client(\"dynamodb\")\n",
    "# response = client.create_table(\n",
    "#     AttributeDefinitions=[\n",
    "#         {\n",
    "#             \"AttributeName\": \"Artist\",\n",
    "#             \"AttributeType\": \"S\",\n",
    "#         },\n",
    "#         {\n",
    "#             \"AttributeName\": \"SongTitle\",\n",
    "#             \"AttributeType\": \"S\",\n",
    "#         },\n",
    "#     ],\n",
    "#     KeySchema=[\n",
    "#         {\n",
    "#             \"AttributeName\": \"Artist\",\n",
    "#             \"KeyType\": \"HASH\",\n",
    "#         },\n",
    "#         {\n",
    "#             \"AttributeName\": \"SongTitle\",\n",
    "#             \"KeyType\": \"RANGE\",\n",
    "#         },\n",
    "#     ],\n",
    "#     ProvisionedThroughput={\n",
    "#         \"ReadCapacityUnits\": 5,\n",
    "#         \"WriteCapacityUnits\": 5,\n",
    "#     },\n",
    "#     TableName=\"Music\",\n",
    "# )\n",
    "waiter = client.get_waiter(\"table_exists\")\n",
    "waiter.wait(TableName=\"Music\", WaiterConfig={\"Delay\": 1, \"MaxAttempts\": 100})\n",
    "response = client.batch_write_item(\n",
    "    RequestItems={\n",
    "        \"Music\": [\n",
    "            {\n",
    "                \"PutRequest\": {\n",
    "                    \"Item\": {\n",
    "                        \"AlbumTitle\": {\n",
    "                            \"S\": \"Somewhat Famous\",\n",
    "                        },\n",
    "                        \"Artist\": {\n",
    "                            \"S\": \"No One You Know\",\n",
    "                        },\n",
    "                        \"SongTitle\": {\n",
    "                            \"S\": \"Call Me Today\",\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"PutRequest\": {\n",
    "                    \"Item\": {\n",
    "                        \"AlbumTitle\": {\n",
    "                            \"S\": \"Songs About Life\",\n",
    "                        },\n",
    "                        \"Artist\": {\n",
    "                            \"S\": \"Acme Band\",\n",
    "                        },\n",
    "                        \"SongTitle\": {\n",
    "                            \"S\": \"Happy Day\",\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"PutRequest\": {\n",
    "                    \"Item\": {\n",
    "                        \"AlbumTitle\": {\n",
    "                            \"S\": \"Blue Sky Blues\",\n",
    "                        },\n",
    "                        \"Artist\": {\n",
    "                            \"S\": \"No One You Know\",\n",
    "                        },\n",
    "                        \"SongTitle\": {\n",
    "                            \"S\": \"Scared of My Shadow\",\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "response = client.put_item(\n",
    "    Item={\n",
    "        \"AlbumTitle\": {\n",
    "            \"S\": \"Somewhat Famous\",\n",
    "        },\n",
    "        \"Artist\": {\n",
    "            \"S\": \"No One You Know\",\n",
    "        },\n",
    "        \"SongTitle\": {\n",
    "            \"S\": \"Call Me Today\",\n",
    "        },\n",
    "    },\n",
    "    ReturnConsumedCapacity=\"TOTAL\",\n",
    "    TableName=\"Music\",\n",
    ")\n",
    "my_dyno = DynamoDB(session)\n",
    "response_get = client.get_item(\n",
    "    Key={\n",
    "        \"Artist\": {\n",
    "            \"S\": \"Acme Band\",\n",
    "        },\n",
    "        \"SongTitle\": {\n",
    "            \"S\": \"Happy Day\",\n",
    "        },\n",
    "    },\n",
    "    TableName=\"Music\",\n",
    ")\n",
    "my_dyno_get = my_dyno.get_item(\n",
    "    key={\n",
    "        \"Artist\": {\n",
    "            \"S\": \"Acme Band\",\n",
    "        },\n",
    "        \"SongTitle\": {\n",
    "            \"S\": \"Happy Day\",\n",
    "        },\n",
    "    },\n",
    "    table_name=\"Music\",\n",
    ")\n",
    "assert my_dyno_get == response_get[\"Item\"]\n",
    "update_response = my_dyno.update_item(\n",
    "    ExpressionAttributeNames={\n",
    "        \"#AT\": \"AlbumTitle\",\n",
    "        \"#Y\": \"Year\",\n",
    "    },\n",
    "    ExpressionAttributeValues={\n",
    "        \":t\": {\n",
    "            \"S\": \"Louder Than Ever\",\n",
    "        },\n",
    "        \":y\": {\n",
    "            \"N\": \"2015\",\n",
    "        },\n",
    "    },\n",
    "    key={\n",
    "        \"Artist\": {\n",
    "            \"S\": \"Acme Band\",\n",
    "        },\n",
    "        \"SongTitle\": {\n",
    "            \"S\": \"Happy Day\",\n",
    "        },\n",
    "    },\n",
    "    ReturnValues=\"ALL_NEW\",\n",
    "    table_name=\"Music\",\n",
    "    UpdateExpression=\"SET #Y = :y, #AT = :t\",\n",
    ")\n",
    "assert update_response[\"Attributes\"] == {\n",
    "    \"AlbumTitle\": {\"S\": \"Louder Than Ever\"},\n",
    "    \"Artist\": {\"S\": \"Acme Band\"},\n",
    "    \"Year\": {\"N\": \"2015\"},\n",
    "    \"SongTitle\": {\"S\": \"Happy Day\"},\n",
    "}\n",
    "scan_response = my_dyno.scan_table(table_name=\"Music\")\n",
    "response = client.scan(TableName=\"Music\")\n",
    "assert response[\"Items\"] == scan_response[\"Items\"]\n",
    "my_dyno_pd = DynamoDBPandas(session)\n",
    "dynamo_table = my_dyno_pd.get_table(table_name=\"Music\")\n",
    "assert dynamo_table.name == \"Music\"\n",
    "my_dyno_pd.put_items(\n",
    "    table_name=\"Music\",\n",
    "    item_list=[\n",
    "        {\"Artist\": \"Nobody\", \"SongTitle\": \"Not Famous\"},\n",
    "        {\"Artist\": \"Nicho\", \"SongTitle\": \"Bad song\"},\n",
    "    ],\n",
    ")\n",
    "assert dynamo_table.get_item(Key={\"Artist\": \"Nobody\", \"SongTitle\": \"Not Famous\"})[\n",
    "    \"Item\"\n",
    "] == {\"Artist\": \"Nobody\", \"SongTitle\": \"Not Famous\"}\n",
    "my_dyno_pd.delete_items(\n",
    "    table_name=\"Music\",\n",
    "    item_list=[\n",
    "        {\"Artist\": \"Nobody\", \"SongTitle\": \"Not Famous\"},\n",
    "        {\"Artist\": \"Nicho\", \"SongTitle\": \"Bad song\"},\n",
    "    ],\n",
    ")\n",
    "after_del_resp = dynamo_table.get_item(\n",
    "    Key={\"Artist\": \"Nobody\", \"SongTitle\": \"Not Famous\"}\n",
    ")\n",
    "assert \"Item\" not in after_del_resp\n",
    "response = client.delete_table(TableName=\"Music\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO move to demo notebook\n",
    "# load_dotenv()\n",
    "redshift_user = os.getenv('REDSHIFT_USER')\n",
    "redshift_password = os.getenv('REDSHIFT_PASSWORD')\n",
    "conn = RedshiftConnector(\n",
    "    iam=True,\n",
    "    database='dev',\n",
    "    db_user=redshift_user,\n",
    "    password=redshift_password,\n",
    "    user=redshift_user,\n",
    "    cluster_identifier='nutrien-insights-redshift-cluster-test',\n",
    "    # host=\n",
    "    # 'nutrien-insights-redshift-cluster-test.cydu5eacdk8z.ca-central-1.redshift.amazonaws.com',\n",
    "    # autocommit=True\n",
    ")\n",
    "\n",
    "# with conn.cursor() as cursor:\n",
    "#     cursor.execute(\"create table category_stage (\\\n",
    "#         catid smallint default 0,\\\n",
    "#         catgroup varchar(10) default 'General', \\\n",
    "#         catname varchar(10) default 'General', \\\n",
    "#         catdesc varchar(50) default 'General');\")\n",
    "# print(conn.redshift_execute_select(table=\"category_stage\"))\n",
    "\n",
    "# print(conn.redshift_execute_select(table='event'))\n",
    "\n",
    "# conn.redshift_execute_insert(\n",
    "#     table='category_stage',\n",
    "#     source_table='category',\n",
    "#     source_columns='catid, catname',\n",
    "#     values=\"(12, 'Concerts', 'Comedy', 'All stand-up comedy performances')\"\n",
    "# )\n",
    "\n",
    "# print(\n",
    "#     conn.redshift_execute_delete(\n",
    "#         table=\"category\",\n",
    "#         reference_tables=\"event\",\n",
    "#         conditions=\"event.catid=category.catid and category.catid=9\"))\n",
    "\n",
    "# print(conn.redshift_execute_select(table='category_stage'))\n",
    "#Connect to the cluster\n",
    "# conn = redshift_connector.connect(\n",
    "#     host=\n",
    "#     'nutrien-insights-redshift-cluster-test.cydu5eacdk8z.ca-central-1.redshift.amazonaws.com',\n",
    "#         database='dev',\n",
    "#         user='awsuser',\n",
    "#         password='Nutrieninsights1'\n",
    "#   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_text = \"\"\"\n",
    "{\n",
    "\"fruit\": {\n",
    "    \"apple\": {\n",
    "        \"size\": \"small\",\n",
    "        \"color\": \"red\",\n",
    "        \"country\": \"USA\"\n",
    "    },\n",
    "    \"banana\":{\n",
    "        \"size\": \"medium\",\n",
    "        \"color\": \"yellow\",\n",
    "        \"country\": \"Fiji\"\n",
    "    },\n",
    "    \"orange\":{\n",
    "        \"size\": \"large\",\n",
    "        \"color\": \"orange\",\n",
    "        \"country\": \"Egypt\"\n",
    "    }\n",
    "}\n",
    "}\n",
    "\"\"\"\n",
    "yaml_text = \"\"\"\n",
    "    name: \"Vishvajit\"\n",
    "    age: 23\n",
    "    address: Noida\n",
    "    Occupation: Software Developer\n",
    "    Skills:\n",
    "    - Python\n",
    "    - Django\n",
    "    - Flask\n",
    "    - FastAPI\n",
    "    - DRF ( Django Rest Framework )\n",
    "\"\"\"\n",
    "toml_text = \"\"\"\n",
    "    [user]\n",
    "    player_x.color = \"blue\"\n",
    "    player_o.color = \"green\"\n",
    "\n",
    "    [constant]\n",
    "    board_size = 3\n",
    "\n",
    "    [server]\n",
    "    url = \"https://tictactoe.example.com\"\n",
    "\"\"\"\n",
    "ini_text = \"\"\"\n",
    "    [apple]\n",
    "    size = small\n",
    "    color = red\n",
    "    country = USA\n",
    "\"\"\"\n",
    "sample_dict = {\n",
    "    \"Vegetable\": {\n",
    "        \"carrot\": {\"size\": \"cylindrical\", \"color\": \"orange\", \"country\": \"America\"}\n",
    "    }\n",
    "}\n",
    "a = ConfigParser(json_text)\n",
    "b = ConfigParser(ini_text)\n",
    "c = ConfigParser(yaml_text)\n",
    "d = ConfigParser(toml_text)\n",
    "print(a.merge_dictionaries(sample_dict))\n",
    "# print(c.parse_yaml())\n",
    "# print(d.parse_toml())\n",
    "# print(b.parse_ini())\n",
    "\n",
    "# from pynutrien.aws.s3 import S3Operations\n",
    "# s3 = S3Operations()\n",
    "# s3.read_object()\n",
    "\n",
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "with s3.open(path) as fh:\n",
    "    data = json.load(fh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-060ea9d4c316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlogger2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Logger' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from time import sleep\n",
    "\n",
    "logger = Logger(__name__)\n",
    "logger2 = Logger()\n",
    "\n",
    "assert logger is logger2\n",
    "\n",
    "logger.info(\"Hello\")\n",
    "logger.warning(\"Hello\")\n",
    "\n",
    "with RuntimeLogger(logger, \"time_this\"):\n",
    "    sleep(0.1)\n",
    "\n",
    "# This should not be done, since it will print the traceback/error for an exception that is handled\n",
    "try:\n",
    "    with RuntimeLogger(logger, \"time_this_2\"):\n",
    "        sleep(0.1)\n",
    "        raise RuntimeError(\"oops\")\n",
    "        sleep(0.1)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "# when debugging, this will print the traceback once for stderr and once for stdout\n",
    "# in the glue environment it will be split into the different output streams\n",
    "with RuntimeLogger(logger, \"time_this_3\"):\n",
    "    sleep(0.1)\n",
    "    raise RuntimeError(\"oops2\")\n",
    "    sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TestETL1(ETLExtendedBase):\n",
    "    job_name = \"TEST_ETL_1\"\n",
    "\n",
    "    def setup(self):\n",
    "        pass\n",
    "\n",
    "    def extract(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self):\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "class TestETL2(ETLExtendedBase):\n",
    "    job_name = \"TEST_ETL_2\"\n",
    "\n",
    "    # def job_name(self): pass\n",
    "\n",
    "    def extract(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self):\n",
    "        raise Exception(\"Test\")\n",
    "\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "j1 = TestETL1()\n",
    "j1.run()\n",
    "\n",
    "j2 = TestETL2()\n",
    "j2.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'standardize_column_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7b7f69ddf5f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m ]))\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize_column_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_df_with_integer_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'standardize_column_names' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# There're 4 scenarios for column names to appear in a dataset\n",
    "# 1. Dataset has headers (read from csv as \"string\" even when it's integer value) -> should work normally\n",
    "# 2. Dataset has headers (directly created from pandas so when it's integer, column name is read as \"integer\")\n",
    "#     -> a TypeError will be raised by the system\n",
    "# 3. Dataset has no headers (read from csv as \"string\" (\"Unnamed\")) -> column names become \"Unnamed\" and work normally\n",
    "# 4. Dataset has no headers (directly created from pandas so an integer will be assigned automatically to name)\n",
    "#     -> a TypeError will be raised by the system\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"tests/std_column_names_test_files/bigmac.csv\")\n",
    "# df = pd.DataFrame([[0,423,2342],[12,123,1231],[1,3,5]])\n",
    "# df = pd.DataFrame({\n",
    "#     0: ['a','b'],\n",
    "#     1: ['c','d'],\n",
    "#     'price': [2,4]\n",
    "# # })\n",
    "# print(df)\n",
    "# df.columns = df.columns.astype(str)\n",
    "# print(type(df.columns))\n",
    "# print(df.columns.dtype)\n",
    "# print(list(df))\n",
    "# print(standardize_column_names(df))\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark_df_with_integer_headers = spark.createDataFrame(data=[\n",
    "        ('a','c',2),\n",
    "        ('b','d',4)\n",
    "    ],\n",
    "    schema = T.StructType([\n",
    "    T.StructField('1         ', T.StringType(), False),\n",
    "    T.StructField('2', T.StringType(), False),\n",
    "    T.StructField('Price   ', T.IntegerType(), False),\n",
    "]))\n",
    "\n",
    "new_df = standardize_column_names(spark_df_with_integer_headers)\n",
    "print(type(new_df))\n",
    "new_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO move\n",
    "# if __name__ == '__main__':\n",
    "# date_range = pd.date_range('2022-11-04', periods=5, freq='D')\n",
    "# test_files = {\n",
    "#         'id': [1, 2, 3, 4, 5],\n",
    "#         'letter_code': ['A','B','C','D','E'],\n",
    "#         'string_date': ['2022-11-04','2022-11-05','2022-11-06','2022-11-07','NaN'],\n",
    "#         'date': date_range\n",
    "#         }\n",
    "# df = pd.DataFrame(test_files)\n",
    "# print(df)\n",
    "\n",
    "# df['date'] = df['date'].dt.strftime('%B %d, %Y')\n",
    "# new_df = delegate_df(df=df, date_column='date', new_format='%B %d, %dxfcsz')\n",
    "# print(new_df)\n",
    "# from pyspark.sql import SparkSession\n",
    "# import pyspark.sql.types as T\n",
    "# from datetime import date\n",
    "# spark = SparkSession.builder.appName('pytest-spark').getOrCreate()\n",
    "# test_files = [\n",
    "#     (1,'A','2022-11-04',date(2022,11,4)),\n",
    "#     (2,'B','2022-11-05',date(2022,11,5)),\n",
    "#     (3,'C','2022-11-06',date(2022,11,6)),\n",
    "#     (4,'D','2022-11-07',date(2022,11,7)),\n",
    "#     (5,'E','2022-11-08',date(2022,11,8)),\n",
    "#     ]\n",
    "# schema = T.StructType([\n",
    "#     T.StructField('id', T.StringType(), False),\n",
    "#     T.StructField('letter_code', T.StringType(), False),\n",
    "#     T.StructField('string_date', T.StringType(), False),\n",
    "#     T.StructField('date', T.DateType(), False)\n",
    "# ])\n",
    "# df = spark.createDataFrame(test_files=test_files, schema=schema)\n",
    "# df.show()\n",
    "# date_df = df.select(\"date\").collect()\n",
    "# date_values = [e.__getattr__(\"date\").strftime('%B %d, %Y') for e in date_df]\n",
    "# print(date_values)\n",
    "# df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "# new = df.withColumn('date',date_format('dt', 'MM/dd/yyy'))\n",
    "# new.show()\n",
    "# new_df = delegate_df(df=df, date_column='string_date',new_format='%B %d, %dxfcsz')\n",
    "# str_date_col = new_df.select(\"string_date\").collect()\n",
    "# delated_date_str_list = [e.__getattr__('string_date') for e in str_date_col]\n",
    "# new_df.show()\n",
    "# print(delated_date_str_list)\n",
    "# des = [\n",
    "#     'November 04, 2022',\n",
    "#     'November 05, 2022',\n",
    "#     'November 06, 2022',\n",
    "#     'November 07, 2022',\n",
    "#     'November 08, 2022']\n",
    "# print(delated_date_str_list == des)\n",
    "\n",
    "# obj_w_date = 'abc'\n",
    "# print(type(date(2022,11,4)))\n",
    "# delegated_obj_w_date = delegate_df(df=obj_w_date, new_format='%B %d, %Y')\n",
    "# print(delegated_obj_w_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO remove\n",
    "import json\n",
    "res = json.load('s3://aws-glue-assets-175633476877-ca-central-1/config/env.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
