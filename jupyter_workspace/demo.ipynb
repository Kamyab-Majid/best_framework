{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "# PyNutrien Demo/Development notebook"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "\n",
                "1) Install AWS CLI and set secrets in `~/.aws/credentials`\n",
                "2) Set profile region in `~/.aws/config` -- TODO see link\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "!pip install -r requirements.txt\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['/home/glue_user/workspace', '/home/glue_user/workspace/jupyter_workspace', '/home/glue_user/aws-glue-libs/PyGlue.zip', '/home/glue_user/spark/python/lib/py4j-0.10.9-src.zip', '/home/glue_user/spark/python', '/usr/lib64/python37.zip', '/usr/lib64/python3.7', '/usr/lib64/python3.7/lib-dynload', '', '/home/glue_user/.local/lib/python3.7/site-packages', '/usr/lib64/python3.7/site-packages', '/usr/lib/python3.7/site-packages', '/home/glue_user/.local/lib/python3.7/site-packages/IPython/extensions', '/home/glue_user/.ipython']\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "#lib_path = str(Path(os.getcwd()).parent.absolute().resolve())\n",
                "#sys.path.insert(0, lib_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "            <div>\n",
                            "                <p><b>SparkSession - hive</b></p>\n",
                            "                \n",
                            "        <div>\n",
                            "            <p><b>SparkContext</b></p>\n",
                            "\n",
                            "            <p><a href=\"http://fd5dbe678052:4040\">Spark UI</a></p>\n",
                            "\n",
                            "            <dl>\n",
                            "              <dt>Version</dt>\n",
                            "                <dd><code>v3.1.1-amzn-0</code></dd>\n",
                            "              <dt>Master</dt>\n",
                            "                <dd><code>local[*]</code></dd>\n",
                            "              <dt>AppName</dt>\n",
                            "                <dd><code>pyspark-shell</code></dd>\n",
                            "            </dl>\n",
                            "        </div>\n",
                            "        \n",
                            "            </div>\n",
                            "        "
                        ],
                        "text/plain": [
                            "<pyspark.sql.session.SparkSession at 0x7f6b4462bbd0>"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pynutrien\n",
                "\n",
                "from pynutrien.aws.glue import GlueSparkContext\n",
                "\n",
                "context = GlueSparkContext()\n",
                "context.spark_session # show SparkUI"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imports and Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "env: AWS_PROFILE=default\n"
                    ]
                }
            ],
            "source": [
                "%env AWS_PROFILE=default"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pynutrien"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Library Usage Examples"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Catalog Reader Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----------+-----------+------+-------------+----------+---------------+--------------+-----+-----------------+------------+-------+----------------+----+\n",
                        "|primary_key|       name|gender|customer_type|birth_date|    personal_id|   card_number|email|     phone_number|country_code|address|meta_last_update|year|\n",
                        "+-----------+-----------+------+-------------+----------+---------------+--------------+-----+-----------------+------------+-------+----------------+----+\n",
                        "|     135567|  T. Oakman|      |       Active|          | SIN: 154720759|Not Applicable|     |   1 822 435 3970|          CA|       |      2008-08-12|2017|\n",
                        "|     135635|B. Kanahele|      |     Prospect|1959-08-20| SIN: 755909975|Not Applicable|     |            -2865|          CA|       |      2008-08-08|2017|\n",
                        "|     135844|  B. Wandel|     M|       Active|1956-06-15|NINO: VT931050C|Not Applicable|     |                 |          CA|       |      2008-01-05|2017|\n",
                        "|     136073|    S. Naff|     M|       Active|1981-06-22| SSN: 817967572|Not Applicable|     |+1 (254) 817 5954|          US|       |      2008-08-04|2017|\n",
                        "|     136304| F. Dolejsi|      |     Prospect|1970-01-31| SSN: 867689822|   5.58434E+15|     |   (101) 525 4703|          US|       |      2008-04-08|2017|\n",
                        "|     136501|   C. Gudis|      |       Active|1991-07-31|NINO: BI502209D|Not Applicable|     |      0787 285898|          CA|       |      2008-03-03|2017|\n",
                        "|     136517|K. Savickas|      |       Active|1971-10-20| SIN: 906953997|   4.18434E+15|     | 1 (236) 068 7996|          CA|       |      2008-04-04|2017|\n",
                        "|     136614|  L. Denmon|      |       Active|1958-09-09| SIN: 616285730|   5.58534E+15|     |                 |          CA|       |      2008-05-09|2017|\n",
                        "|     136620|    S. Mott|      |     Prospect|1962-12-10| SSN: 565252427|Not Applicable|     |   1 361 671 7691|          US|       |      2008-06-02|2017|\n",
                        "|     136654|  G. Watkin|      |       Active|1963-02-13| SSN: 761634015|Not Applicable|     |      16623613508|          US|       |      2008-06-02|2017|\n",
                        "|     136757|   M. Luman|      |       Active|          |NINO: XQ494342D|Not Applicable|     |                 |          CA|       |      2008-01-05|2017|\n",
                        "|     136780| R. Burrier|      |     Prospect|       N/A|NINO: WX221478B|Not Applicable|     |    059 2556 6016|          CA|       |      2008-11-07|2017|\n",
                        "|     136999|   T. Rhame|      |     Prospect|          |NINO: RS587408B|   5.58534E+15|     |                 |          CA|       |      2008-02-06|2017|\n",
                        "|     137028|    A. Toti|      |       Active|          |               |   5.58534E+15|     |   (398) 274 7233|          CA|       |      2008-04-08|2017|\n",
                        "|     137041|  R. Maciag|      |       Active|1966-04-13| SIN: 017333360|   5.58534E+15|     |                 |          CA|       |      2008-09-09|2017|\n",
                        "|     137141|  G. Holien|      |       Active|1978-11-24| SSN: 325525732|Not Applicable|     |           -10369|          US|       |      2008-09-09|2017|\n",
                        "|     137286|  H. Shanor|      |       Active|1996-06-26| SSN: 736656754|   5.58634E+15|     |   1-989-077-9775|          US|       |      2008-01-09|2017|\n",
                        "|     137403|    M. Yust|      |     Prospect|       N/A| SIN: 489616383|   5.58634E+15|     |   1 970 510 9387|          CA|       |      2008-01-01|2017|\n",
                        "|     137451| S. Blazing|      |       Active|1985-06-06| SSN: 508179127|   4.18634E+15|     |   (009) 408 7671|          US|       |      2008-11-03|2017|\n",
                        "|     137476| M. Lashmet|      |       Active|1959-02-23| SSN: 700051470|Not Applicable|     |                 |          US|       |      2008-09-09|2017|\n",
                        "+-----------+-----------+------+-------------+----------+---------------+--------------+-----+-----------------+------------+-------+----------------+----+\n",
                        "only showing top 20 rows\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "from pynutrien.aws.glue import GlueSparkContext\n",
                "from pynutrien.aws.glue.catalog import GlueReader\n",
                "\n",
                "context = GlueSparkContext()\n",
                "Reader = GlueReader(context.glue_context, None)\n",
                "df = (\n",
                "    Reader.catalog(\"crawler_test\", \"caseware_customer_a\")\n",
                "        .transformation_ctx(\"cases\")\n",
                "        .dataframe()\n",
                ")\n",
                "df.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Job"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add arguments\n",
                "import sys\n",
                "sys.argv.extend([\"--cfg_file_path\", \"dummy\", \"--env_file_path\", \"dummy\", \"--abc\", \"123\"])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This code defines a custom GlueJob class called MyGlueJob that inherits from the GlueJob class. The MyGlueJob class has several properties, including the job_name and arguments properties, and defines three methods: extract, transform, and load.\n",
                "\n",
                "The extract method uses the Glue context to create a dynamic frame from a specified input path in JSON format. The transform and load methods currently do not have any implementation.\n",
                "\n",
                "To run the job, an instance of the MyGlueJob class is created and the run method is called on it. This will run the extract, transform, and load methods in sequence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2022-12-09 16:50:45.706 [INFO] [logger:75] MainThread:TEST_GLUE - Starting: TEST_GLUE\n",
                        "2022-12-09 16:50:45.707 [INFO] [logger:75] MainThread:TEST_GLUE - Starting: setup\n",
                        "2022-12-09 16:50:45.711 [INFO] [job:71] MainThread:TEST_GLUE - Library Version: '1.0.0'\n",
                        "2022-12-09 16:50:45.714 [INFO] [job:72] MainThread:TEST_GLUE - Supplied Arguments: ['/home/glue_user/.local/lib/python3.7/site-packages/ipykernel_launcher.py', '--ip=127.0.0.1', '--stdin=9019', '--control=9017', '--hb=9016', '--Session.signature_scheme=\"hmac-sha256\"', '--Session.key=b\"8fa504da-70d4-4fc4-b848-34bfccb8d45d\"', '--shell=9018', '--transport=\"tcp\"', '--iopub=9020', '--f=/home/glue_user/.local/share/jupyter/runtime/kernel-v2-708vv2DaeU9VqT4.json', '--cfg_file_path', 'dummy', '--env_file_path', 'dummy', '--abc', '123']\n",
                        "2022-12-09 16:50:45.716 [INFO] [job:73] MainThread:TEST_GLUE - Parsed Arguments: {'job_bookmark_option': 'job-bookmark-disable', 'job_bookmark_from': None, 'job_bookmark_to': None, 'JOB_ID': None, 'JOB_RUN_ID': None, 'SECURITY_CONFIGURATION': None, 'encryption_type': None, 'enable_data_lineage': None, 'RedshiftTempDir': None, 'TempDir': None, 'abc': '123', 'env_file_path': 'dummy', 'cfg_file_path': 'dummy'}\n",
                        "2022-12-09 16:50:45.773 [INFO] [job:74] MainThread:TEST_GLUE - Spark Config: [('spark.eventLog.enabled', 'true'), ('spark.network.crypto.keyLength', '256'), ('spark.network.crypto.enabled', 'true'), ('spark.network.crypto.keyFactoryAlgorithm', 'PBKDF2WithHmacSHA256'), ('spark.driver.port', '46047'), ('spark.io.encryption.enabled', 'false'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.deployMode', 'client'), ('spark.authenticate.secret', '7b2c9dfc-f27a-4cd4-9455-870274d304b7'), ('spark.sql.warehouse.dir', 'file:/home/glue_user/workspace/jupyter_workspace/spark-warehouse'), ('spark.app.startTime', '1670604623259'), ('spark.executor.extraClassPath', '/home/glue_user/spark/jars/*:/home/glue_user/aws-glue-libs/jars/*'), ('spark.executor.id', 'driver'), ('spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs', 'false'), ('spark.unsafe.sorter.spill.read.ahead.enabled', 'false'), ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'), ('spark.app.name', 'pyspark-shell'), ('spark.history.fs.logDirectory', 'file:////tmp/spark-events'), ('spark.sql.catalogImplementation', 'hive'), ('spark.rdd.compress', 'True'), ('spark.network.crypto.saslFallback', 'false'), ('spark.submit.pyFiles', ''), ('spark.app.id', 'local-1670604624412'), ('spark.driver.extraClassPath', '/home/glue_user/spark/jars/*:/home/glue_user/aws-glue-libs/jars/*'), ('spark.driver.host', 'b31e7ffe8f09'), ('spark.ui.showConsoleProgress', 'true'), ('spark.authenticate', 'true')]\n",
                        "2022-12-09 16:50:45.801 [INFO] [job:97] MainThread:TEST_GLUE - Skipping parsing config file: dummy\n",
                        "2022-12-09 16:50:45.802 [INFO] [job:109] MainThread:TEST_GLUE - Parsed Env: {}\n",
                        "2022-12-09 16:50:45.803 [INFO] [job:97] MainThread:TEST_GLUE - Skipping parsing config file: dummy\n",
                        "2022-12-09 16:50:45.806 [INFO] [job:111] MainThread:TEST_GLUE - Parsed Config: {}\n",
                        "2022-12-09 16:50:45.808 [INFO] [logger:79] MainThread:TEST_GLUE - End: setup [0.101s]\n",
                        "2022-12-09 16:50:45.813 [INFO] [logger:75] MainThread:TEST_GLUE - Starting: extract\n",
                        "2022-12-09 16:50:46.156 [INFO] [logger:79] MainThread:TEST_GLUE - End: extract [0.343s]\n",
                        "2022-12-09 16:50:46.158 [INFO] [logger:75] MainThread:TEST_GLUE - Starting: transform\n",
                        "2022-12-09 16:50:46.159 [INFO] [logger:79] MainThread:TEST_GLUE - End: transform [0.001s]\n",
                        "2022-12-09 16:50:46.162 [INFO] [logger:75] MainThread:TEST_GLUE - Starting: load\n",
                        "2022-12-09 16:50:46.163 [INFO] [logger:79] MainThread:TEST_GLUE - End: load [0.002s]\n",
                        "2022-12-09 16:50:46.165 [INFO] [logger:75] MainThread:TEST_GLUE - Starting: cleanup\n",
                        "2022-12-09 16:50:46.172 [INFO] [logger:79] MainThread:TEST_GLUE - End: cleanup [0.008s]\n",
                        "2022-12-09 16:50:46.174 [INFO] [logger:79] MainThread:TEST_GLUE - End: TEST_GLUE [0.467s]\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "from pynutrien.aws.glue import GlueJob\n",
                "\n",
                "class MyGlueJob(GlueJob):\n",
                "    job_name = \"TEST_GLUE\"\n",
                "    arguments = [\"abc\"]\n",
                "    input_path=\"s3://awsglue-datasets/examples/us-legislators/all/persons.json\"\n",
                "\n",
                "    def __init__(self, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "\n",
                "    def extract(self,):\n",
                "        \n",
                "        self.dynamicframe = self.glue_context.create_dynamic_frame.from_options(\n",
                "            connection_type=\"s3\",\n",
                "            connection_options={\"paths\": [self.input_path], \"recurse\": True},\n",
                "            format=\"json\",\n",
                "        )\n",
                "\n",
                "    def transform(self):\n",
                "        pass\n",
                "\n",
                "    def load(self):\n",
                "        pass\n",
                "    \n",
                "    \n",
                "\n",
                "job = MyGlueJob()\n",
                "job.run()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### DynamoDB"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This code uses the boto3 library to interact with Amazon DynamoDB. The code first creates a Session object and uses it to create a client for DynamoDB. It then uses the waiter object to wait for the Music table to be created before inserting some items into the table using the batch_write_item method. The code also inserts a single item using the put_item method and retrieves an item using the get_item method.\n",
                "\n",
                "Next, the code creates an instance of the DynamoDB class, which is a custom class for interacting with DynamoDB. It uses this instance to retrieve the same item as before and to update an item in the table using the update_item method. It also uses the scan_table method to scan the entire Music table.\n",
                "\n",
                "Finally, the code creates an instance of the DynamoDBPandas class, which is a custom class for interacting with DynamoDB tables as Pandas dataframes. It uses this instance to get a dynamo_table object for the Music table and to insert, update, and delete items from the table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2022-12-09 16:56:33.632 [INFO] [credentials:1223] MainThread:botocore.credentials - Found credentials in shared credentials file: ~/.aws/credentials\n"
                    ]
                }
            ],
            "source": [
                "import boto3\n",
                "from pynutrien.aws.dynamodb import DynamoDB,DynamoDBPandas\n",
                "\n",
                "session = boto3.session.Session()\n",
                "client = session.client(\"dynamodb\")\n",
                "response = client.create_table(\n",
                "    AttributeDefinitions=[\n",
                "        {\n",
                "            'AttributeName': 'Artist',\n",
                "            'AttributeType': 'S',\n",
                "        },\n",
                "        {\n",
                "            'AttributeName': 'SongTitle',\n",
                "            'AttributeType': 'S',\n",
                "        },\n",
                "    ],\n",
                "    KeySchema=[\n",
                "        {\n",
                "            'AttributeName': 'Artist',\n",
                "            'KeyType': 'HASH',\n",
                "        },\n",
                "        {\n",
                "            'AttributeName': 'SongTitle',\n",
                "            'KeyType': 'RANGE',\n",
                "        },\n",
                "    ],\n",
                "    ProvisionedThroughput={\n",
                "        'ReadCapacityUnits': 5,\n",
                "        'WriteCapacityUnits': 5,\n",
                "    },\n",
                "    TableName='Music',\n",
                ")\n",
                "waiter = client.get_waiter(\"table_exists\")\n",
                "waiter.wait(TableName=\"Music\", WaiterConfig={\"Delay\": 1, \"MaxAttempts\": 100})\n",
                "response = client.batch_write_item(\n",
                "    RequestItems={\n",
                "        \"Music\": [\n",
                "            {\n",
                "                \"PutRequest\": {\n",
                "                    \"Item\": {\n",
                "                        \"AlbumTitle\": {\n",
                "                            \"S\": \"Somewhat Famous\",\n",
                "                        },\n",
                "                        \"Artist\": {\n",
                "                            \"S\": \"No One You Know\",\n",
                "                        },\n",
                "                        \"SongTitle\": {\n",
                "                            \"S\": \"Call Me Today\",\n",
                "                        },\n",
                "                    },\n",
                "                },\n",
                "            },\n",
                "            {\n",
                "                \"PutRequest\": {\n",
                "                    \"Item\": {\n",
                "                        \"AlbumTitle\": {\n",
                "                            \"S\": \"Songs About Life\",\n",
                "                        },\n",
                "                        \"Artist\": {\n",
                "                            \"S\": \"Acme Band\",\n",
                "                        },\n",
                "                        \"SongTitle\": {\n",
                "                            \"S\": \"Happy Day\",\n",
                "                        },\n",
                "                    },\n",
                "                },\n",
                "            },\n",
                "            {\n",
                "                \"PutRequest\": {\n",
                "                    \"Item\": {\n",
                "                        \"AlbumTitle\": {\n",
                "                            \"S\": \"Blue Sky Blues\",\n",
                "                        },\n",
                "                        \"Artist\": {\n",
                "                            \"S\": \"No One You Know\",\n",
                "                        },\n",
                "                        \"SongTitle\": {\n",
                "                            \"S\": \"Scared of My Shadow\",\n",
                "                        },\n",
                "                    },\n",
                "                },\n",
                "            },\n",
                "        ],\n",
                "    },\n",
                ")\n",
                "response = client.put_item(\n",
                "    Item={\n",
                "        \"AlbumTitle\": {\n",
                "            \"S\": \"Somewhat Famous\",\n",
                "        },\n",
                "        \"Artist\": {\n",
                "            \"S\": \"No One You Know\",\n",
                "        },\n",
                "        \"SongTitle\": {\n",
                "            \"S\": \"Call Me Today\",\n",
                "        },\n",
                "    },\n",
                "    ReturnConsumedCapacity=\"TOTAL\",\n",
                "    TableName=\"Music\",\n",
                ")\n",
                "my_dyno = DynamoDB(session)\n",
                "response_get = client.get_item(\n",
                "    Key={\n",
                "        \"Artist\": {\n",
                "            \"S\": \"Acme Band\",\n",
                "        },\n",
                "        \"SongTitle\": {\n",
                "            \"S\": \"Happy Day\",\n",
                "        },\n",
                "    },\n",
                "    TableName=\"Music\",\n",
                ")\n",
                "my_dyno_get = my_dyno.get_item(\n",
                "    key={\n",
                "        \"Artist\": {\n",
                "            \"S\": \"Acme Band\",\n",
                "        },\n",
                "        \"SongTitle\": {\n",
                "            \"S\": \"Happy Day\",\n",
                "        },\n",
                "    },\n",
                "    table_name=\"Music\",\n",
                ")\n",
                "assert my_dyno_get == response_get[\"Item\"]\n",
                "update_response = my_dyno.update_item(\n",
                "    ExpressionAttributeNames={\n",
                "        \"#AT\": \"AlbumTitle\",\n",
                "        \"#Y\": \"Year\",\n",
                "    },\n",
                "    ExpressionAttributeValues={\n",
                "        \":t\": {\n",
                "            \"S\": \"Louder Than Ever\",\n",
                "        },\n",
                "        \":y\": {\n",
                "            \"N\": \"2015\",\n",
                "        },\n",
                "    },\n",
                "    key={\n",
                "        \"Artist\": {\n",
                "            \"S\": \"Acme Band\",\n",
                "        },\n",
                "        \"SongTitle\": {\n",
                "            \"S\": \"Happy Day\",\n",
                "        },\n",
                "    },\n",
                "    ReturnValues=\"ALL_NEW\",\n",
                "    table_name=\"Music\",\n",
                "    UpdateExpression=\"SET #Y = :y, #AT = :t\",\n",
                ")\n",
                "assert update_response[\"Attributes\"] == {\n",
                "    \"AlbumTitle\": {\"S\": \"Louder Than Ever\"},\n",
                "    \"Artist\": {\"S\": \"Acme Band\"},\n",
                "    \"Year\": {\"N\": \"2015\"},\n",
                "    \"SongTitle\": {\"S\": \"Happy Day\"},\n",
                "}\n",
                "scan_response = my_dyno.scan_table(table_name=\"Music\")\n",
                "response = client.scan(TableName=\"Music\")\n",
                "assert response[\"Items\"] == scan_response[\"Items\"]\n",
                "my_dyno_pd = DynamoDBPandas(session)\n",
                "dynamo_table = my_dyno_pd.get_table(table_name=\"Music\")\n",
                "assert dynamo_table.name == \"Music\"\n",
                "my_dyno_pd.put_items(\n",
                "    table_name=\"Music\",\n",
                "    item_list=[\n",
                "        {\"Artist\": \"Nobody\", \"SongTitle\": \"Not Famous\"},\n",
                "        {\"Artist\": \"Nicho\", \"SongTitle\": \"Bad song\"},\n",
                "    ],\n",
                ")\n",
                "assert dynamo_table.get_item(Key={\"Artist\": \"Nobody\", \"SongTitle\": \"Not Famous\"})[\n",
                "    \"Item\"\n",
                "] == {\"Artist\": \"Nobody\", \"SongTitle\": \"Not Famous\"}\n",
                "my_dyno_pd.delete_items(\n",
                "    table_name=\"Music\",\n",
                "    item_list=[\n",
                "        {\"Artist\": \"Nobody\", \"SongTitle\": \"Not Famous\"},\n",
                "        {\"Artist\": \"Nicho\", \"SongTitle\": \"Bad song\"},\n",
                "    ],\n",
                ")\n",
                "after_del_resp = dynamo_table.get_item(\n",
                "    Key={\"Artist\": \"Nobody\", \"SongTitle\": \"Not Famous\"}\n",
                ")\n",
                "assert \"Item\" not in after_del_resp\n",
                "response = client.delete_table(TableName=\"Music\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Redshift"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This code uses the RedshiftConnector class to connect to an Amazon Redshift cluster. The RedshiftConnector class takes several arguments, including the iam flag to indicate whether to use IAM authentication, the name of the database to connect to, the username and password for the database user, and the identifier for the Redshift cluster.\n",
                "\n",
                "Once the RedshiftConnector instance is created, you can use it to interact with the Redshift cluster. For example, you could use the query method to execute SQL queries on the cluster, or the fetch_df method to fetch the results of a query as a Pandas dataframe."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The following code works in case you have `REDSHIFT_USER` and `REDSHIFT_PASSWORD` defined in your `.env` fuke in the current working directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "ename": "RuntimeError",
                    "evalue": "Missing required arguments. Please make sure arguments and values exist for                 'database','db_user', 'password', 'user', 'cluster_identifier'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-18-315513650550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredshift_password\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredshift_user\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mcluster_identifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nutrien-insights-redshift-cluster-test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/workspace/jupyter_workspace/pynutrien/aws/redshift.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             self.connection = (\n\u001b[1;32m     57\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_connection_using_IAM_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_connection_using_AWS_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             )\n",
                        "\u001b[0;32m~/workspace/jupyter_workspace/pynutrien/aws/redshift.py\u001b[0m in \u001b[0;36mcreate_connection_using_IAM_credentials\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m         raise RuntimeError(\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m\"\u001b[0m\u001b[0mMissing\u001b[0m \u001b[0mrequired\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mmake\u001b[0m \u001b[0msure\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mexist\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0;34m'database'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'db_user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'password'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cluster_identifier'\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         )\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mRuntimeError\u001b[0m: Missing required arguments. Please make sure arguments and values exist for                 'database','db_user', 'password', 'user', 'cluster_identifier'"
                    ]
                }
            ],
            "source": [
                "from dotenv import load_dotenv\n",
                "from pynutrien.aws.redshift import RedshiftConnector  \n",
                "load_dotenv()\n",
                "redshift_user = os.getenv('REDSHIFT_USER')\n",
                "redshift_password = os.getenv('REDSHIFT_PASSWORD')\n",
                "conn = RedshiftConnector(\n",
                "    iam=True,\n",
                "    database='dev',\n",
                "    db_user=redshift_user,\n",
                "    password=redshift_password,\n",
                "    user=redshift_user,\n",
                "    cluster_identifier='nutrien-insights-redshift-cluster-test',\n",
                ")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Config"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This code defines four string variables containing sample data in JSON, YAML, TOML, and INI formats, respectively. It also defines a sample_dict dictionary.\n",
                "\n",
                "Next, the code creates four instances of the ConfigParser class, passing each instance the corresponding text data. The ConfigParser class is a custom class that appears to provide methods for parsing and merging configuration data from various formats.\n",
                "\n",
                "The code then calls the merge_dictionaries method on the first ConfigParser instance and passes it the sample_dict as an argument. This would merge the data from json_text with the data from sample_dict and return the resulting dictionary.\n",
                "\n",
                "The code also contains commented out code that appears to use the S3Operations and S3FileSystem classes to read data from an Amazon S3 bucket, but this code is not executed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'fruit': {'apple': {'size': 'small', 'color': 'red', 'country': 'USA'}, 'banana': {'size': 'medium', 'color': 'yellow', 'country': 'Fiji'}, 'orange': {'size': 'large', 'color': 'orange', 'country': 'Egypt'}}}\n",
                        "{'apple': {'size': 'small', 'color': 'red', 'country': 'USA'}}\n",
                        "{'name': 'Vishvajit', 'age': 23, 'address': 'Noida', 'Occupation': 'Software Developer', 'Skills': ['Python', 'Django', 'Flask', 'FastAPI', 'DRF ( Django Rest Framework )']}\n",
                        "{'user': {'player_x': {'color': 'blue'}, 'player_o': {'color': 'green'}}, 'constant': {'board_size': 3}, 'server': {'url': 'https://tictactoe.example.com'}}\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "from pynutrien.core.config import ConfigParser\n",
                "json_text = \"\"\"\n",
                "{\n",
                "\"fruit\": {\n",
                "    \"apple\": {\n",
                "        \"size\": \"small\",\n",
                "        \"color\": \"red\",\n",
                "        \"country\": \"USA\"\n",
                "    },\n",
                "    \"banana\":{\n",
                "        \"size\": \"medium\",\n",
                "        \"color\": \"yellow\",\n",
                "        \"country\": \"Fiji\"\n",
                "    },\n",
                "    \"orange\":{\n",
                "        \"size\": \"large\",\n",
                "        \"color\": \"orange\",\n",
                "        \"country\": \"Egypt\"\n",
                "    }\n",
                "}\n",
                "}\n",
                "\"\"\"\n",
                "yaml_text = \"\"\"\n",
                "    name: \"Vishvajit\"\n",
                "    age: 23\n",
                "    address: Noida\n",
                "    Occupation: Software Developer\n",
                "    Skills:\n",
                "    - Python\n",
                "    - Django\n",
                "    - Flask\n",
                "    - FastAPI\n",
                "    - DRF ( Django Rest Framework )\n",
                "\"\"\"\n",
                "toml_text = \"\"\"\n",
                "    [user]\n",
                "    player_x.color = \"blue\"\n",
                "    player_o.color = \"green\"\n",
                "\n",
                "    [constant]\n",
                "    board_size = 3\n",
                "\n",
                "    [server]\n",
                "    url = \"https://tictactoe.example.com\"\n",
                "\"\"\"\n",
                "ini_text = \"\"\"\n",
                "    [apple]\n",
                "    size = small\n",
                "    color = red\n",
                "    country = USA\n",
                "\"\"\"\n",
                "sample_dict = {\n",
                "    \"Vegetable\": {\n",
                "        \"carrot\": {\"size\": \"cylindrical\", \"color\": \"orange\", \"country\": \"America\"}\n",
                "    }\n",
                "}\n",
                "my_config_parser=ConfigParser()\n",
                "print(my_config_parser.parse_json(json_text))\n",
                "print(my_config_parser.parse_ini(ini_text))\n",
                "print(my_config_parser.parse_yaml(yaml_text))\n",
                "print(my_config_parser.parse_toml(toml_text))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Logger"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This code imports the sleep function from the time module, creates two Logger objects, and uses a RuntimeLogger context manager to time the execution of a block of code.\n",
                "\n",
                "The Logger class is used to create logger objects that can be used to log messages at different levels (e.g., info, warning, etc.). The __name__ argument passed to the Logger constructor determines the logger's name. If the __name__ argument is not provided, the logger's name is set to \"\" (an empty string).\n",
                "\n",
                "In this code, the first Logger object is created with the __name__ argument set to __name__, which will be the name of the module in which this code is executed. The second Logger object is created without any arguments, so its name will be an empty string.\n",
                "\n",
                "The assert statement at the beginning of the code checks that the two Logger objects are the same object (i.e., they are references to the same instance of the Logger class). This is because the Logger class uses the singleton design pattern, which ensures that only one instance of the class is created and returned whenever the Logger constructor is called.\n",
                "\n",
                "The logger and logger2 objects are then used to log messages at the info and warning levels.\n",
                "\n",
                "The RuntimeLogger context manager is used to time the execution of a block of code. The RuntimeLogger constructor takes two arguments: a logger object, and a label that specifies the name of the operation being timed. The with statement is used to create a RuntimeLogger context, which starts a timer when the block of code inside the with statement is entered and stops the timer when the block is exited."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2022-12-09 17:04:30.637 [INFO] [<ipython-input-24-d89f52f7d5d6>:8] MainThread:__main__ - Hello\n",
                        "2022-12-09 17:04:30.649 [WARNING] [<ipython-input-24-d89f52f7d5d6>:9] MainThread:__main__ - Hello\n",
                        "2022-12-09 17:04:30.657 [INFO] [logger:75] MainThread:__main__ - Starting: time_this\n",
                        "2022-12-09 17:04:30.774 [INFO] [logger:79] MainThread:__main__ - End: time_this [0.116s]\n",
                        "2022-12-09 17:04:30.776 [INFO] [logger:75] MainThread:__main__ - Starting: time_this_2\n",
                        "2022-12-09 17:04:30.886 [INFO] [logger:89] MainThread:__main__ - End: time_this_2 [0.110s] with error\n",
                        "2022-12-09 17:04:30.888 [ERROR] [logger:93] MainThread:__main__ - Exception Raised: RuntimeError('oops')\n",
                        "2022-12-09 17:04:30.893 [INFO] [logger:75] MainThread:__main__ - Starting: time_this_3\n",
                        "2022-12-09 17:04:30.994 [INFO] [logger:89] MainThread:__main__ - End: time_this_3 [0.101s] with error\n",
                        "2022-12-09 17:04:30.996 [ERROR] [logger:93] MainThread:__main__ - Exception Raised: RuntimeError('oops2')\n"
                    ]
                },
                {
                    "ename": "RuntimeError",
                    "evalue": "oops2",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-24-d89f52f7d5d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mRuntimeLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"time_this_3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"oops2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mRuntimeError\u001b[0m: oops2"
                    ]
                }
            ],
            "source": [
                "\n",
                "from time import sleep\n",
                "from pynutrien.core.logger import Logger,RuntimeLogger\n",
                "logger = Logger(__name__)\n",
                "logger2 = Logger()\n",
                "\n",
                "assert logger is logger2\n",
                "\n",
                "logger.info(\"Hello\")\n",
                "logger.warning(\"Hello\")\n",
                "\n",
                "with RuntimeLogger(logger, \"time_this\"):\n",
                "    sleep(0.1)\n",
                "\n",
                "# This should not be done, since it will print the traceback/error for an exception that is handled\n",
                "try:\n",
                "    with RuntimeLogger(logger, \"time_this_2\"):\n",
                "        sleep(0.1)\n",
                "        raise RuntimeError(\"oops\")\n",
                "        sleep(0.1)\n",
                "except RuntimeError:\n",
                "    pass\n",
                "\n",
                "# when debugging, this will print the traceback once for stderr and once for stdout\n",
                "# in the glue environment it will be split into the different output streams\n",
                "with RuntimeLogger(logger, \"time_this_3\"):\n",
                "    sleep(0.1)\n",
                "    raise RuntimeError(\"oops2\")\n",
                "    sleep(0.1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ETL"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This code defines two classes, TestETL1 and TestETL2, that inherit from the ETLExtendedBase class. Both classes define the setup, extract, transform, and load methods that are required by the ETLExtendedBase class. The TestETL1 class also defines a job_name attribute, while the TestETL2 class defines a job_name method instead of an attribute.\n",
                "\n",
                "The TestETL1 and TestETL2 classes are subclasses of the ETLExtendedBase class and are therefore examples of the ETL (extract-transform-load) design pattern. In the ETL design pattern, data is extracted from one or more sources, transformed into a format that is suitable for analysis or storage, and then loaded into a destination, such as a database or a data warehouse.\n",
                "\n",
                "The ETLExtendedBase class is an abstract base class that defines the methods and attributes that are common to all ETL jobs. It defines the setup, extract, transform, and load methods, which are used to perform the four main steps of the ETL process. It also defines the job_name attribute, which specifies the name of the ETL job."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2022-12-09 17:05:24.560 [INFO] [logger:75] MainThread:__main__ - Starting: TEST_ETL_1\n",
                        "2022-12-09 17:05:24.563 [INFO] [logger:75] MainThread:__main__ - Starting: setup\n",
                        "2022-12-09 17:05:24.566 [INFO] [logger:79] MainThread:__main__ - End: setup [0.002s]\n",
                        "2022-12-09 17:05:24.571 [INFO] [logger:75] MainThread:__main__ - Starting: extract\n",
                        "2022-12-09 17:05:24.572 [INFO] [logger:79] MainThread:__main__ - End: extract [0.001s]\n",
                        "2022-12-09 17:05:24.578 [INFO] [logger:75] MainThread:__main__ - Starting: transform\n",
                        "2022-12-09 17:05:24.579 [INFO] [logger:79] MainThread:__main__ - End: transform [0.001s]\n",
                        "2022-12-09 17:05:24.580 [INFO] [logger:75] MainThread:__main__ - Starting: load\n",
                        "2022-12-09 17:05:24.581 [INFO] [logger:79] MainThread:__main__ - End: load [0.001s]\n",
                        "2022-12-09 17:05:24.583 [INFO] [logger:79] MainThread:__main__ - End: TEST_ETL_1 [0.023s]\n",
                        "2022-12-09 17:05:24.584 [INFO] [logger:75] MainThread:__main__ - Starting: TEST_ETL_2\n",
                        "2022-12-09 17:05:24.585 [INFO] [logger:75] MainThread:__main__ - Starting: extract\n",
                        "2022-12-09 17:05:24.593 [INFO] [logger:79] MainThread:__main__ - End: extract [0.008s]\n",
                        "2022-12-09 17:05:24.594 [INFO] [logger:75] MainThread:__main__ - Starting: transform\n",
                        "2022-12-09 17:05:24.595 [INFO] [logger:89] MainThread:__main__ - End: transform [0.001s] with error\n",
                        "2022-12-09 17:05:24.596 [ERROR] [logger:93] MainThread:__main__ - Exception Raised: Exception('Test')\n",
                        "2022-12-09 17:05:24.596 [INFO] [logger:89] MainThread:__main__ - End: TEST_ETL_2 [0.012s] with error\n",
                        "2022-12-09 17:05:24.601 [ERROR] [logger:91] MainThread:__main__ - Exception Raised: Exception('Test')\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/home/glue_user/workspace/jupyter_workspace/pynutrien/etl/base.py\", line 42, in run\n",
                        "    fn()\n",
                        "  File \"<ipython-input-25-7eb8e00e635c>\", line 27, in transform\n",
                        "    raise Exception(\"Test\")\n",
                        "Exception: Test\n"
                    ]
                },
                {
                    "ename": "Exception",
                    "evalue": "Test",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-25-7eb8e00e635c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mj2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestETL2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mj2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m~/workspace/jupyter_workspace/pynutrien/etl/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mRuntimeLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m<ipython-input-25-7eb8e00e635c>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mException\u001b[0m: Test"
                    ]
                }
            ],
            "source": [
                "from pynutrien.etl import ETLExtendedBase\n",
                "\n",
                "class TestETL1(ETLExtendedBase):\n",
                "    job_name = \"TEST_ETL_1\"\n",
                "\n",
                "    def setup(self):\n",
                "        pass\n",
                "\n",
                "    def extract(self):\n",
                "        pass\n",
                "\n",
                "    def transform(self):\n",
                "        pass\n",
                "\n",
                "    def load(self):\n",
                "        pass\n",
                "\n",
                "class TestETL2(ETLExtendedBase):\n",
                "    job_name = \"TEST_ETL_2\"\n",
                "\n",
                "    # def job_name(self): pass\n",
                "\n",
                "    def extract(self):\n",
                "        pass\n",
                "\n",
                "    def transform(self):\n",
                "        raise Exception(\"Test\")\n",
                "\n",
                "    def load(self):\n",
                "        pass\n",
                "\n",
                "j1 = TestETL1()\n",
                "j1.run()\n",
                "\n",
                "j2 = TestETL2()\n",
                "j2.run()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Column"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This code creates a PySpark DataFrame with three columns and two rows of data, then passes the DataFrame to a function called standardize_column_names to clean up the column names. The resulting DataFrame is printed out, along with its schema (i.e. the data types of its columns)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pyspark.sql.dataframe.DataFrame'>\n",
                        "root\n",
                        " |-- 1: string (nullable = false)\n",
                        " |-- 2: string (nullable = false)\n",
                        " |-- price: integer (nullable = false)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "import pyspark.sql.types as T\n",
                "from pyspark.sql import SparkSession\n",
                "from pynutrien.util.column import standardize_column_names\n",
                "spark = SparkSession.builder.getOrCreate()\n",
                "spark_df_with_integer_headers = spark.createDataFrame(data=[\n",
                "        ('a','c',2),\n",
                "        ('b','d',4)\n",
                "    ],\n",
                "    schema = T.StructType([\n",
                "    T.StructField('1         ', T.StringType(), False),\n",
                "    T.StructField('2', T.StringType(), False),\n",
                "    T.StructField('Price   ', T.IntegerType(), False),\n",
                "]))\n",
                "\n",
                "new_df = standardize_column_names(spark_df_with_integer_headers)\n",
                "print(type(new_df))\n",
                "new_df.printSchema()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        },
        "vscode": {
            "interpreter": {
                "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
